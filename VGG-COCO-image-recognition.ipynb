{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c9fb5f",
   "metadata": {},
   "source": [
    "Object Feature detection using vgg16 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acef20f",
   "metadata": {},
   "source": [
    "## Import required Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4be70997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import random\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "import torchvision.transforms as original_transforms\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import multiprocessing as mp\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf12c6d",
   "metadata": {},
   "source": [
    "## initalize GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4147c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of CPUs: 16\n",
      "Device in use: cpu\n",
      "Found 0 GPU Device/s.\n"
     ]
    }
   ],
   "source": [
    "n_gpus = torch.cuda.device_count()\n",
    "USING_CPU = not torch.cuda.is_available()\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if (torch.cuda.is_available()  and n_gpus > 0) else \"cpu\")\n",
    "kwargs = {'num_workers': mp.cpu_count() , 'pin_memory': True} if DEVICE.type=='cuda' else {'num_workers': mp.cpu_count()//2, 'prefetch_factor': 4}\n",
    "\n",
    "print(f'Num of CPUs: {mp.cpu_count()}')\n",
    "print(f'Device in use: {DEVICE}')\n",
    "print(f'Found {n_gpus} GPU Device/s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca0e43",
   "metadata": {},
   "source": [
    "## Set and create data loader for COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "598cb4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMG_DIR = './images/val2017'\n",
    "TRAIN_ANN_FILE = './images/instances_val2017.json'\n",
    "USE_PRETRAINED = True\n",
    "SAVED_MODEL_PATH = \"./model/ssd300_vgg16_checkpoint\"\n",
    "\n",
    "def load_dataset(transform):\n",
    "    return dset.CocoDetection(root = TRAIN_IMG_DIR, \n",
    "                              annFile = TRAIN_ANN_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8add801",
   "metadata": {},
   "source": [
    "## create image augumentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a28850ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fucntions to modify the images (flip/mirror/resize)\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        self.hf = transforms.RandomHorizontalFlip(1)\n",
    "        \n",
    "    def __call__(self, img, bboxes):\n",
    "        if torch.rand(1)[0] < self.p:            \n",
    "            img = self.hf.forward(img)\n",
    "            bboxes = self.hf.forward(bboxes)\n",
    "        return img, bboxes\n",
    "    \n",
    "    \n",
    "class RandomVerticalFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        self.vf = transforms.RandomVerticalFlip(1)\n",
    "        \n",
    "    def __call__(self, img, bboxes):\n",
    "        if torch.rand(1)[0] < self.p:                    \n",
    "            img = self.vf.forward(img)\n",
    "            bboxes = self.vf.forward(bboxes)\n",
    "        return img, bboxes\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.resize = transforms.Resize(self.size, antialias=True)\n",
    "        \n",
    "    def __call__(self, img, bboxes):\n",
    "        img = self.resize.forward(img)\n",
    "        bboxes = self.resize.forward(bboxes)\n",
    "        return img, bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc2a57",
   "metadata": {},
   "source": [
    "## Function to display image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "507cb552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(sample):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from torchvision.transforms.v2 import functional as F\n",
    "    from torchvision.utils import draw_bounding_boxes\n",
    "    \n",
    "    resize = Resize((300, 300))\n",
    "    \n",
    "    rhf = RandomHorizontalFlip()\n",
    "    rvf = RandomVerticalFlip()\n",
    "    image, target = sample\n",
    "    \n",
    "    image, bboxes = image,target[\"boxes\"] \n",
    "    \n",
    "    image, bboxes = resize(image, bboxes)\n",
    "    image, bboxes = rhf(image, bboxes)\n",
    "    image, bboxes = rvf(image, bboxes)\n",
    "    \n",
    "    if isinstance(image, PIL.Image.Image):\n",
    "        image = F.to_image_tensor(image)\n",
    "        \n",
    "    image = F.convert_dtype(image, torch.uint8)\n",
    "    annotated_image = draw_bounding_boxes(image, bboxes, colors=\"yellow\", width=3)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(annotated_image.permute(1, 2, 0).numpy())\n",
    "    ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6518bafc",
   "metadata": {},
   "source": [
    "## Transformation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6019e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomPhotometricDistort(),        \n",
    "        transforms.RandomAutocontrast(),\n",
    "        transforms.RandomEqualize(),\n",
    "        transforms.GaussianBlur(kernel_size=3),\n",
    "        transforms.ToImageTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2fc6e5",
   "metadata": {},
   "source": [
    "## Create Dataset using the wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1b4a670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=14.73s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_train = load_dataset(transform=transform)\n",
    "coco_train = dset.wrap_dataset_for_transforms_v2(coco_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c916e446",
   "metadata": {},
   "source": [
    "## Create dataset Class to apply tranforms to samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68f8c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewCocoDataset(Dataset):    \n",
    "    def __init__(self, coco_dataset, image_size=(312, 312)):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            coco_dataset (dataset): The coco dataset containing all the expected transforms.\n",
    "            image_size (tuple): Target image size. Default is (512, 512)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.coco_dataset = coco_dataset\n",
    "        self.resize = Resize(image_size)\n",
    "        self.rhf = RandomHorizontalFlip()\n",
    "        self.rvf = RandomVerticalFlip()   \n",
    "        self.transformer = transforms.Compose([\n",
    "            transforms.ToImageTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "        ])\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.coco_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        new_target = {}\n",
    "        \n",
    "        image, target = self.coco_dataset[idx]\n",
    "        \n",
    "        if 'boxes' not in target:    \n",
    "            new_idx = idx-1\n",
    "            _img, _t = self.coco_dataset[new_idx]\n",
    "            while 'boxes' not in _t :\n",
    "                new_idx -= 1\n",
    "                _img, _t = self.coco_dataset[new_idx]\n",
    "                \n",
    "            image, target = self.coco_dataset[new_idx]\n",
    "        \n",
    "        \n",
    "        image, bboxes = image, target[\"boxes\"] \n",
    "            \n",
    "        image, bboxes = self.resize(image, bboxes)\n",
    "        image, bboxes = self.rhf(image, bboxes)\n",
    "        image, bboxes = self.rvf(image, bboxes)\n",
    "        \n",
    "        image = self.transformer(image)\n",
    "        \n",
    "        new_boxes = []\n",
    "        for box in bboxes:\n",
    "            if box[0] < box[2] and box[1] < box[3]:\n",
    "                new_boxes.append(box)\n",
    "        \n",
    "        new_target[\"boxes\"] = torch.stack(new_boxes)\n",
    "        new_target[\"labels\"] = target[\"labels\"]\n",
    "    \n",
    "        return (image, new_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6eccf1",
   "metadata": {},
   "source": [
    "## Create Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03ce2a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_coco_train = NewCocoDataset(coco_train)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    new_coco_train,\n",
    "    batch_size=20,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
    "     **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e50f9",
   "metadata": {},
   "source": [
    "## Get names and corresponding indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e78d3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=15.53s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import pycocotools.coco\n",
    "\n",
    "coco_anns = pycocotools.coco.COCO(TRAIN_ANN_FILE)\n",
    "catIDs = coco_anns.getCatIds()\n",
    "cats = coco_anns.loadCats(catIDs)\n",
    "\n",
    "name_idx = {}\n",
    "\n",
    "for sub_dict in cats:\n",
    "    name_idx[sub_dict[\"id\"]] = sub_dict[\"name\"]\n",
    "    \n",
    "del coco_anns, catIDs, cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aafdfd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(data_loader))\n",
    "if USING_CPU:\n",
    "    x = torch.stack(data[0])\n",
    "else:\n",
    "    x = data[0]\n",
    "print(x.shape)\n",
    "\n",
    "plt.imshow(data[0][0].permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee6499",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ryan Eakin\\Desktop\\COCO-feature-based-ML\\VGG-COCO-image-recognition.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ryan%20Eakin/Desktop/COCO-feature-based-ML/VGG-COCO-image-recognition.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ryan%20Eakin/Desktop/COCO-feature-based-ML/VGG-COCO-image-recognition.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape, data[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data[1][0]['boxes']\n",
    "data[0][0].shape, data[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9142723f",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58373d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = models.get_model(\"ssd300_vgg16\", weights=None, weights_backbone=None).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad2bb3",
   "metadata": {},
   "source": [
    "## Initialize Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65d0afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "base_model.apply(weights_init)\n",
    "if (DEVICE.type == 'cuda') and (n_gpus > 1):\n",
    "    base_model = nn.DataParallel(base_model, list(range(n_gpus)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add32f01",
   "metadata": {},
   "source": [
    "## Display Loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e2c6585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSD(\n",
       "  (backbone): SSDFeatureExtractorVGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "    )\n",
       "    (extra): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "      (3-4): 2 x Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (anchor_generator): DefaultBoxGenerator(aspect_ratios=[[2], [2, 3], [2, 3], [2, 3], [2], [2]], clip=True, scales=[0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05], steps=[8, 16, 32, 64, 100, 300])\n",
       "  (head): SSDHead(\n",
       "    (classification_head): SSDClassificationHead(\n",
       "      (module_list): ModuleList(\n",
       "        (0): Conv2d(512, 364, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(1024, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(512, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4-5): 2 x Conv2d(256, 364, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (regression_head): SSDRegressionHead(\n",
       "      (module_list): ModuleList(\n",
       "        (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4-5): 2 x Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.48235, 0.45882, 0.40784], std=[0.00392156862745098, 0.00392156862745098, 0.00392156862745098])\n",
       "      Resize(min_size=(300,), max_size=300, mode='bilinear')\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d58ed1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35,641,826 total parameters.\n",
      "35,641,826 training parameters.\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in base_model.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in base_model.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748452a",
   "metadata": {},
   "source": [
    "## Set model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04faee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(base_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6ad69c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PRETRAINED:\n",
    "    new_LR = 1e-5 # change this value to set a new Learning Rate for the version of notebook\n",
    "    \n",
    "    if USING_CPU:\n",
    "        checkpoint = torch.load(SAVED_MODEL_PATH, map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        checkpoint = torch.load(SAVED_MODEL_PATH)\n",
    "        \n",
    "    base_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = new_LR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a37e2a",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc382e08",
   "metadata": {},
   "source": [
    "## Set epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4aa7b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f1c8499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe8d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5915 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    running_classifier_loss = 0.0\n",
    "    running_bbox_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    counter = 0\n",
    "    base_model.train()\n",
    "    \n",
    "    for data_point in tqdm(data_loader):\n",
    "        _i, _t = data_point[0], data_point[1]\n",
    "        \n",
    "        if USING_CPU:\n",
    "            _i = torch.stack(_i)\n",
    "\n",
    "        _i = _i.to(DEVICE)\n",
    "        _t = [{k: v.to(DEVICE) for k, v in __t.items()} for __t in _t]\n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = base_model(_i, _t)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += losses.item()\n",
    "        del loss_dict, losses\n",
    "        counter += 1\n",
    "        if counter % 500 == 499:\n",
    "            last_classifier_loss = running_classifier_loss / 500 # loss per batch\n",
    "            last_bbox_loss = running_bbox_loss / 500 # loss per batch\n",
    "            last_loss = running_loss / 500 # loss per batch\n",
    "            print(f'Epoch {epoch}, Batch {counter + 1}, Running Loss: {last_loss}')\n",
    "            running_classifier_loss = 0.0\n",
    "            running_bbox_loss = 0.0\n",
    "            running_loss = 0.0\n",
    "            \n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5ec541",
   "metadata": {},
   "source": [
    "## test model against validation set and display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11298682",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_IMG_DIR = '/images/val2017'\n",
    "VAL_ANN_FILE = './images/instances_val2017.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47a4fc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.50s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def load_val_dataset(transform):\n",
    "    return dset.CocoDetection(root = VAL_IMG_DIR, \n",
    "                              annFile = VAL_ANN_FILE)\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToImageTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "    ]\n",
    ")\n",
    "coco_val = load_val_dataset(transform=val_transform)\n",
    "coco_val = dset.wrap_dataset_for_transforms_v2(coco_val)\n",
    "\n",
    "new_coco_val = NewCocoDataset(coco_val)\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    new_coco_val,\n",
    "    batch_size=50 if not USING_CPU else 8,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
    "     **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4f7f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dtype_converter = transforms.ConvertImageDtype(torch.uint8)\n",
    "data = next(iter(val_data_loader))\n",
    "\n",
    "_i = data[0]\n",
    "\n",
    "threshold = 0.5\n",
    "idx = 3\n",
    "\n",
    "if USING_CPU:\n",
    "    _i = torch.stack(_i)\n",
    "\n",
    "_i = _i.to(DEVICE)\n",
    "base_model.eval()\n",
    "p_t = base_model(_i)\n",
    "\n",
    "confidence_length = len(np.argwhere(p_t[idx]['scores'] > threshold)[0])\n",
    "\n",
    "p_boxes = p_t[idx]['boxes'][: confidence_length]\n",
    "p_labels = [name_idx[i] for i in p_t[idx]['labels'][: confidence_length].tolist()]\n",
    "i_img = img_dtype_converter(_i[idx])\n",
    "\n",
    "annotated_image = draw_bounding_boxes(i_img, p_boxes, p_labels, colors=\"yellow\", width=3)\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(annotated_image.permute(1, 2, 0).numpy())\n",
    "ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1479c554",
   "metadata": {},
   "source": [
    "## Save and load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3927d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './model/ssd300_vgg16_checkpoint'\n",
    "\n",
    "torch.save({\n",
    "            'epoch': EPOCHS,\n",
    "            'model_state_dict': base_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce5b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(PATH)\n",
    "base_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
